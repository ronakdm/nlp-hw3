{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"A3_senteiment_classification.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM3hlyJRiSxwKYvY1BupqtW"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OsMecSDeJER6","executionInfo":{"status":"ok","timestamp":1612580659151,"user_tz":300,"elapsed":1220,"user":{"displayName":"Ronak Mehta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvL0OEKDdfWJnnCVVtFTsqSBT6-uRCtSdS5vM7MA=s64","userId":"12475517112718652159"}},"outputId":"aa9d33fb-c7c5-4936-8452-1aa5ab4c8b52"},"source":["from collections import Counter\n","import os\n","import random\n","import tarfile\n","import tempfile\n","import urllib.request\n","\n","import nltk\n","nltk.download('punkt')\n","\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from tqdm import tqdm\n","\n","import pickle\n","from sklearn.metrics import f1_score"],"execution_count":1,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9xCIv-x_nElR"},"source":["# Sentiment Classification on Pretrained GloVe Embeddings\n","\n","The following trains an RNN to classify sentiment of text, using a pretrained embedding layer. This code is largely the same as the example notebook given in class: https://colab.research.google.com/drive/14GAMb7c6FbDnhWvqcliCZ8KYNvqdnQz7?usp=sharing#scrollTo=uofVxyhMbVtx\n","\n","Key changes are highlighted in the text."]},{"cell_type":"markdown","metadata":{"id":"fB34be2tq4qr"},"source":["**Here, we generate the embedding matrix by using GloVe embeddings for words from the sentiment dataset that were also in the GloVe vocabulary, while using random weights for the other words.**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zDNz1EEcpNSj","executionInfo":{"status":"ok","timestamp":1612580659352,"user_tz":300,"elapsed":265,"user":{"displayName":"Ronak Mehta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvL0OEKDdfWJnnCVVtFTsqSBT6-uRCtSdS5vM7MA=s64","userId":"12475517112718652159"}},"outputId":"3d01d836-1c8b-4322-fc2b-4a0398e30271"},"source":["! git clone https://github.com/ronakdm/nlp-hw3.git"],"execution_count":2,"outputs":[{"output_type":"stream","text":["fatal: destination path 'nlp-hw3' already exists and is not an empty directory.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"w7fYpprYq-Gs","executionInfo":{"status":"ok","timestamp":1612580956469,"user_tz":300,"elapsed":256,"user":{"displayName":"Ronak Mehta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvL0OEKDdfWJnnCVVtFTsqSBT6-uRCtSdS5vM7MA=s64","userId":"12475517112718652159"}}},"source":["def create_embedding_weights(token_to_idx):\n","    embeddings = pickle.load(open(\"nlp-hw3/embeddings.p\", \"rb\")).numpy()\n","    pretrain_word_list = pickle.load(open(\"nlp-hw3/vocab_list.p\", \"rb\"))\n","\n","    pretrain_size, embed_dim = embeddings.shape\n","\n","    vocab_size = len(token_to_idx)\n","\n","    # Weight matrix of new embedding.\n","    weights = torch.rand(vocab_size, embed_dim)\n","\n","    # Find out which of the GloVe words are in the IMDB vocabulary.\n","    # Find their indices, and replace the indices of the matrix with those embeddings.\n","    count = 0\n","    in_vocab_indices = []\n","    for i, word in enumerate(pretrain_word_list):\n","        if word in token_to_idx:\n","            idx = token_to_idx[word]\n","            weights[idx, :] = torch.tensor(embeddings[i, :])\n","            in_vocab_indices.append(idx)\n","            count += 1\n","    in_vocab_indices = torch.tensor(in_vocab_indices)\n","\n","    print(\"%d word types from the GloVe embeddings were found in the %d size IMDB review vocabulary.\" % (count, vocab_size))\n","\n","    return weights, in_vocab_indices"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QqCZq1B4ZG7z"},"source":["**Hyperparameters below. Note that these are picked to prioritize computational cheapness and performance can definitely be increased with a larger network and more epochs of training.**"]},{"cell_type":"code","metadata":{"id":"HtG-7SR_KICq","executionInfo":{"status":"ok","timestamp":1612580670306,"user_tz":300,"elapsed":247,"user":{"displayName":"Ronak Mehta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvL0OEKDdfWJnnCVVtFTsqSBT6-uRCtSdS5vM7MA=s64","userId":"12475517112718652159"}}},"source":["MAX_SEQ_LEN = -1  # -1 for no truncation\n","UNK_THRESHOLD = 5\n","BATCH_SIZE = 64\n","N_EPOCHS = 3\n","LEARNING_RATE = 1e-3\n","HIDDEN_DIM = 128\n","N_RNN_LAYERS = 2\n","\n","PAD = \"@@PAD@@\"\n","UNK = \"@@UNK@@\""],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"ThEoro0vKJ6D","executionInfo":{"status":"ok","timestamp":1612580670905,"user_tz":300,"elapsed":208,"user":{"displayName":"Ronak Mehta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvL0OEKDdfWJnnCVVtFTsqSBT6-uRCtSdS5vM7MA=s64","userId":"12475517112718652159"}}},"source":["def seed_everything(seed=1):\n","    random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"0n5TAA-yk5ga","executionInfo":{"status":"ok","timestamp":1612580672236,"user_tz":300,"elapsed":213,"user":{"displayName":"Ronak Mehta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvL0OEKDdfWJnnCVVtFTsqSBT6-uRCtSdS5vM7MA=s64","userId":"12475517112718652159"}}},"source":["def download_data():\n","    \"\"\"\n","    A function to download and uncompress the imdb data. You don't have to understand anything here.\n","    \"\"\"\n","\n","    def extract_data(dir, split):\n","        data = []\n","        for label in (\"pos\", \"neg\"):\n","            label_dir = os.path.join(dir, \"aclImdb\", split, label)\n","            files = sorted(os.listdir(label_dir))\n","            for file in files:\n","                filepath = os.path.join(label_dir, file)\n","                with open(filepath, encoding=\"UTF-8\") as f:\n","                    data.append({\"raw\": f.read(), \"label\": label})\n","        return data\n","\n","    url = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n","    stream = urllib.request.urlopen(url)\n","    tar = tarfile.open(fileobj=stream, mode=\"r|gz\")\n","    with tempfile.TemporaryDirectory() as td:\n","        tar.extractall(path=td)\n","        train_data = extract_data(td, \"train\")\n","        test_data = extract_data(td, \"test\")\n","        return train_data, test_data\n","\n","\n","def split_data(train_data, num_split=2000):\n","    \"\"\"Splits the training data into training and development sets.\"\"\"\n","    random.shuffle(train_data)\n","    return train_data[:-num_split], train_data[-num_split:]"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"-ecX-sTck8Ok","executionInfo":{"status":"ok","timestamp":1612580675053,"user_tz":300,"elapsed":262,"user":{"displayName":"Ronak Mehta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvL0OEKDdfWJnnCVVtFTsqSBT6-uRCtSdS5vM7MA=s64","userId":"12475517112718652159"}}},"source":["def tokenize(data, max_seq_len=MAX_SEQ_LEN):\n","    \"\"\"\n","    Here we use nltk to tokenize data. There are many othe possibilities. We also truncate the\n","    sequences so that the training time and memory is more manageable. You can think of truncation\n","    as making a decision only looking at the first X words.\n","    \"\"\"\n","    for example in data:\n","        example[\"text\"] = []\n","        for sent in nltk.sent_tokenize(example[\"raw\"]):\n","            example[\"text\"].extend(nltk.word_tokenize(sent))\n","        if max_seq_len >= 0:\n","            example[\"text\"] = example[\"text\"][:max_seq_len]\n","\n","\n","def create_vocab(data, unk_threshold=UNK_THRESHOLD):\n","    \"\"\"\n","    Creates a vocabulary with tokens that have frequency above unk_threshold and assigns each token\n","    a unique index, including the special tokens.\n","    \"\"\"\n","    counter = Counter(token for example in data for token in example[\"text\"])\n","    vocab = {token for token in counter if counter[token] > unk_threshold}\n","    print(f\"Vocab size: {len(vocab) + 2}\")  # add the special tokens\n","    print(f\"Most common tokens: {counter.most_common(10)}\")\n","    token_to_idx = {PAD: 0, UNK: 1}\n","    for token in vocab:\n","        token_to_idx[token] = len(token_to_idx)\n","    return token_to_idx\n","\n","\n","def apply_vocab(data, token_to_idx):\n","    \"\"\"\n","    Applies the vocabulary to the data and maps the tokenized sentences to vocab indices as the\n","    model input.\n","    \"\"\"\n","    for example in data:\n","        example[\"text\"] = [token_to_idx.get(token, token_to_idx[UNK]) for token in example[\"text\"]]\n","\n","\n","def apply_label_map(data, label_to_idx):\n","    \"\"\"Converts string labels to indices.\"\"\"\n","    for example in data:\n","        example[\"label\"] = label_to_idx[example[\"label\"]]"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"LVR-J3zGk_N6","executionInfo":{"status":"ok","timestamp":1612580676871,"user_tz":300,"elapsed":215,"user":{"displayName":"Ronak Mehta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvL0OEKDdfWJnnCVVtFTsqSBT6-uRCtSdS5vM7MA=s64","userId":"12475517112718652159"}}},"source":["class SentimentDataset(Dataset):\n","    def __init__(self, data, pad_idx):\n","        data = sorted(data, key=lambda example: len(example[\"text\"]))\n","        self.texts = [example[\"text\"] for example in data]\n","        self.labels = [example[\"label\"] for example in data]\n","        self.pad_idx = pad_idx\n","\n","    def __getitem__(self, index):\n","        return [self.texts[index], self.labels[index]]\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def collate_fn(self, batch):\n","        def tensorize(elements, dtype):\n","            return [torch.tensor(element, dtype=dtype) for element in elements]\n","\n","        def pad(tensors):\n","            \"\"\"Assumes 1-d tensors.\"\"\"\n","            max_len = max(len(tensor) for tensor in tensors)\n","            padded_tensors = [\n","                F.pad(tensor, (0, max_len - len(tensor)), value=self.pad_idx) for tensor in tensors\n","            ]\n","            return padded_tensors\n","\n","        texts, labels = zip(*batch)\n","        return [\n","            torch.stack(pad(tensorize(texts, torch.long)), dim=0),\n","            torch.stack(tensorize(labels, torch.long), dim=0),\n","        ]"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jDR8RHxdZf68"},"source":["**The `SequenceClassifier` now takes `weights` upon initialization instead of an embedding dimension.**."]},{"cell_type":"code","metadata":{"id":"nG8SWhg_lYws","executionInfo":{"status":"ok","timestamp":1612580678510,"user_tz":300,"elapsed":281,"user":{"displayName":"Ronak Mehta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvL0OEKDdfWJnnCVVtFTsqSBT6-uRCtSdS5vM7MA=s64","userId":"12475517112718652159"}}},"source":["class SequenceClassifier(nn.Module):\n","    def __init__(self, vocab_size, weights, hidden_dim, n_labels, n_rnn_layers, pad_idx):\n","        super().__init__()\n","\n","        self.pad_idx = pad_idx\n","\n","        self.embedding = nn.Embedding.from_pretrained(weights, freeze=False)\n","\n","        embedding_dim = weights.shape[1]\n","        \n","        self.rnn = nn.GRU(\n","            embedding_dim, hidden_dim, num_layers=n_rnn_layers, batch_first=True, bidirectional=True\n","        )\n","        # We take the final hidden state at all GRU layers as the sequence representation.\n","        # 2 because bidirectional.\n","        layered_hidden_dim = hidden_dim * n_rnn_layers * 2\n","        self.output = nn.Linear(layered_hidden_dim, n_labels)\n","\n","    def forward(self, text):\n","        # text shape: (batch_size, max_seq_len) where max_seq_len is the max length *in this batch*\n","        # lens shape: (batch_size,)\n","        non_padded_positions = text != self.pad_idx\n","        lens = non_padded_positions.sum(dim=1)\n","\n","        # embedded shape: (batch_size, max_seq_len, embedding_dim)\n","        embedded = self.embedding(text)\n","        # You can pass the embeddings directly to the RNN, but as the input potentially has\n","        # different lengths, how do you know when to stop unrolling the recurrence for each example?\n","        # pytorch provides a util function pack_padded_sequence that converts padded sequences with\n","        # potentially different lengths into a special PackedSequence object that keeps track of\n","        # these things. When passing a PackedSequence object into the RNN, the output will be a\n","        # PackedSequence too (but not the hidden state as that always has a length of 1). Since we\n","        # do not use the per-token output, we do not unpack it. But if you need it, e.g. for\n","        # token-level classification such as POS tagging, you can use pad_packed_sequence to convert\n","        # it back to a regular tensor.\n","        packed_embedded = nn.utils.rnn.pack_padded_sequence(\n","            embedded, lens.cpu(), batch_first=True, enforce_sorted=False\n","        )\n","        # nn.GRU produces two outputs: one is the per-token output and the other is per-sequence.\n","        # The pers-sequence output is simiar to the last per-token output, except that it is taken\n","        # at all layers.\n","        # output (after unpacking) shape: (batch_size, max_seq_len, hidden_dim)\n","        # hidden shape: (n_layers * n_directions, batch_size, hidden_dim)\n","        packed_output, hidden = self.rnn(packed_embedded)\n","        # shape: (batch_size, n_layers * n_directions * hidden_dim)\n","        hidden = hidden.transpose(0, 1).reshape(hidden.shape[1], -1)\n","        # Here we directly output the raw scores without softmax normalization which would produce\n","        # a valid probability distribution. This is because:\n","        # (1) during training, pytorch provides a loss function \"F.cross_entropy\" that combines\n","        # \"log_softmax + F.nll_loss\" in one step. See the `train` function below.\n","        # (2) during evaluation, we usually only care about the class with the highest score, but\n","        # not the actual probablity distribution.\n","        # shape: (batch_size, n_labels)\n","        return self.output(hidden)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"rpltqciDlbRx","executionInfo":{"status":"ok","timestamp":1612580680195,"user_tz":300,"elapsed":257,"user":{"displayName":"Ronak Mehta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvL0OEKDdfWJnnCVVtFTsqSBT6-uRCtSdS5vM7MA=s64","userId":"12475517112718652159"}}},"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aRygDM2zZtW1"},"source":["**Because the computational requirements of this model are not too prohibitive, we employ a hack to \"freeze\" the embeddings that were taken from the GloVe vocabulary. After backward propagation, we zero out the gradients of those weights, so the optimizer makes no step in those dimensions. This is if a tensor indicating which indices are in the GloVe vocabulary is passed to `train`. Additionally, `evaluate` outputs Macro F1 Score as well.**"]},{"cell_type":"code","metadata":{"id":"vRpbKSGileVf","executionInfo":{"status":"ok","timestamp":1612580986126,"user_tz":300,"elapsed":264,"user":{"displayName":"Ronak Mehta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvL0OEKDdfWJnnCVVtFTsqSBT6-uRCtSdS5vM7MA=s64","userId":"12475517112718652159"}}},"source":["def train(model, dataloader, optimizer, device, in_vocab_indices = None):\n","    for texts, labels in tqdm(dataloader):\n","        texts, labels = texts.to(device), labels.to(device)\n","        output = model(texts)\n","        loss = F.cross_entropy(output, labels)\n","        model.zero_grad()\n","        loss.backward()\n","\n","        if in_vocab_indices is not None:\n","            # We would like to freeze the embeddings of the pretrained vectors.\n","            model.embedding.weight.grad[in_vocab_indices] = 0\n","        optimizer.step()\n","\n","\n","def evaluate(model, dataloader, device):\n","    count = correct = 0.0\n","    with torch.no_grad():\n","        for texts, labels in tqdm(dataloader):\n","            texts, labels = texts.to(device), labels.to(device)\n","            # shape: (batch_size, n_labels)\n","            output = model(texts)\n","            # shape: (batch_size,)\n","            predicted = output.argmax(dim=-1)\n","            count += len(predicted)\n","            correct += (predicted == labels).sum().item()\n","    print(f\"Accuracy: {correct / count}\")\n","    print(f\"Macro F1 Score: {f1_score(predicted.cpu(), labels.cpu())}\")"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R-TP6SEVlgoW","executionInfo":{"status":"ok","timestamp":1612580820404,"user_tz":300,"elapsed":125018,"user":{"displayName":"Ronak Mehta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvL0OEKDdfWJnnCVVtFTsqSBT6-uRCtSdS5vM7MA=s64","userId":"12475517112718652159"}},"outputId":"1eecd9ff-0c5e-4961-8fe3-6726f18b08d7"},"source":["seed_everything()\n","\n","print(\"Downloading data\")\n","train_data, test_data = download_data()\n","train_data, dev_data = split_data(train_data)\n","print(f\"Data sample: {train_data[:3]}\")\n","print(f\"train {len(train_data)}, dev {len(dev_data)}, test {len(test_data)}\")\n","\n","print(\"Processing data\")\n","for data in (train_data, dev_data, test_data):\n","    tokenize(data)\n","\n","# Here we only use the training data to create the vocabulary because\n","# (1) we shouldn't look at the test set; and\n","# (2) we want the dev set to accurately reflect the test set performance.\n","# There are people who do other things.\n","token_to_idx = create_vocab(train_data)\n","label_to_idx = {\"neg\": 0, \"pos\": 1}\n","for data in (train_data, dev_data, test_data):\n","    apply_vocab(data, token_to_idx)\n","    apply_label_map(data, label_to_idx)\n","\n","pad_idx = token_to_idx[PAD]\n","train_dataset = SentimentDataset(train_data, pad_idx)\n","dev_dataset = SentimentDataset(dev_data, pad_idx)\n","test_dataset = SentimentDataset(test_data, pad_idx)\n","train_dataloader = DataLoader(\n","    train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=train_dataset.collate_fn\n",")\n","dev_dataloader = DataLoader(\n","    dev_dataset, batch_size=BATCH_SIZE, collate_fn=dev_dataset.collate_fn\n",")\n","test_dataloader = DataLoader(\n","    test_dataset, batch_size=BATCH_SIZE, collate_fn=test_dataset.collate_fn\n",")"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Downloading data\n","Data sample: [{'raw': \"This film is quite boring. There are snippets of naked flesh tossed around in a lame attempt to keep the viewer awake but they don't succeed.<br /><br />The best thing about the movie is Lena Olin--she does a masterful job handling her character, but Day-Lewis garbles most of his lines.<br /><br />Kaufman clearly had no idea how to film this. The incongruities in bouncing between domestic household/marriage issues and political crises are badly matched. Character attitudes change without explanation throughout. Badly disjointed.\", 'label': 'neg'}, {'raw': 'How can you tell that a horror movie is terrible? when you can\\'t stop laughing about it of course! The plot has been well covered by other reviewers, so I\\'ll just add a few things on the hilarity of it all.<br /><br />Some reviews have placed the location in South America, others in Africa, I thought it was in some random island in the Pacific. Where exactly does this take place, seems to be a mystery. The cannibal tribe is conformed by a couple of black women some black men, and a man who looks like a young Frank Zappa banging the drums... the Devil God is a large black man with a terrible case of pink eyes.<br /><br />One of the \"freakiest\" moments in the film is when, \"Pablito\" find his partner hanging from a tree covered in what seems to be an orange substance that I assume is blood, starts screaming for minutes on and on (that\\'s actually funny), and then the head of his partner falls in the ground and \"Pablito\" kicks it a bit for what I assume is \"shits n\\' giggles\" and the eyes actually move...<br /><br />But, of course, then the \"freak\" is gone when you realize the eyes moved because the movie is just bad...<br /><br />I hadn\\'t laughed like this in a loooong while, and I definitely recommend this film for a Sunday afternoon with your friends and you have nothing to do... grab a case of beers and start watching this film, you\\'ll love it! If you are looking for a real horror or gore movie, though... don\\'t\\' bother.', 'label': 'neg'}, {'raw': 'This is a good movie, although people unfamiliar with the Modesty Blaise comics and books may find it a little slow and lacking in action. For the Modesty fan, the movie will be very enjoyable, particularly because it is very faithful in its presentation of the Modesty Blaise \"history\". Peter O\\'Donnell is listed in the credits as \"Creative Consultant\" and the film makers must have actually paid attention to him as the plot follows quite closely the details that have been presented in the comic books over the years {although the events have been recast to modern days). The only thing that the true fan may find disappointing is that there is no Willie Garvin in the story. This lack of Willie is again just being faithful to the Modesty Blaise chronology since the movie takes place in the very early days of Modesty\\'s career. Alexandra Staden makes a very believable young Modesty who actually looks a lot like Modesty is supposed to look. A welcome change from the travesty of the Monica Vitti portrayal of Modesty.', 'label': 'pos'}]\n","train 23000, dev 2000, test 25000\n","Processing data\n","Vocab size: 30595\n","Most common tokens: [('the', 265723), (',', 253416), ('.', 217777), ('and', 143620), ('a', 143303), ('of', 131877), ('to', 122337), ('is', 100309), ('/', 94102), ('>', 94041)]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-2YzifIdn3U1","executionInfo":{"status":"ok","timestamp":1612580965311,"user_tz":300,"elapsed":4916,"user":{"displayName":"Ronak Mehta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvL0OEKDdfWJnnCVVtFTsqSBT6-uRCtSdS5vM7MA=s64","userId":"12475517112718652159"}},"outputId":"3f09808a-e098-4675-e784-318f9b4a7d5d"},"source":["weights, in_vocab_indices = create_embedding_weights(token_to_idx)\n","\n","model = SequenceClassifier(\n","    len(token_to_idx), weights, HIDDEN_DIM, len(label_to_idx), N_RNN_LAYERS, pad_idx\n",")\n","print(f\"Model has {count_parameters(model)} parameters.\")\n","\n","# Adam is just a fancier version of SGD.\n","optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["18741 word types from the GloVe embeddings were found in the 30595 size IMDB review vocabulary.\n","Model has 1965464 parameters.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["SequenceClassifier(\n","  (embedding): Embedding(30595, 50)\n","  (rnn): GRU(50, 128, num_layers=2, batch_first=True, bidirectional=True)\n","  (output): Linear(in_features=512, out_features=2, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"qTBj2BtyaS-t"},"source":["**Full training loop.**"]},{"cell_type":"code","metadata":{"id":"DIehobNjljPz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612581179697,"user_tz":300,"elapsed":190284,"user":{"displayName":"Ronak Mehta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvL0OEKDdfWJnnCVVtFTsqSBT6-uRCtSdS5vM7MA=s64","userId":"12475517112718652159"}},"outputId":"1fbaca80-95d9-414b-d335-882f7a2f44f2"},"source":["try:\n","  print(f\"Random baseline\")\n","  evaluate(model, dev_dataloader, device)\n","  for epoch in range(N_EPOCHS):\n","      print(f\"Epoch {epoch + 1}\")  # 0-based -> 1-based\n","      train(model, train_dataloader, optimizer, device, in_vocab_indices = in_vocab_indices)\n","      evaluate(model, dev_dataloader, device)\n","  print(f\"Test set performance\")\n","  evaluate(model, test_dataloader, device)\n","except KeyboardInterrupt:\n","  print('Graceful Exit')"],"execution_count":20,"outputs":[{"output_type":"stream","text":[" 25%|██▌       | 8/32 [00:00<00:00, 75.97it/s]"],"name":"stderr"},{"output_type":"stream","text":["Random baseline\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 32/32 [00:00<00:00, 36.00it/s]\n","  0%|          | 1/360 [00:00<00:44,  8.00it/s]"],"name":"stderr"},{"output_type":"stream","text":["Accuracy: 0.491\n","Macro F1 Score: 0.26666666666666666\n","Epoch 1\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 360/360 [00:58<00:00,  6.14it/s]\n","100%|██████████| 32/32 [00:00<00:00, 37.11it/s]\n","  0%|          | 1/360 [00:00<01:02,  5.79it/s]"],"name":"stderr"},{"output_type":"stream","text":["Accuracy: 0.8275\n","Macro F1 Score: 0.888888888888889\n","Epoch 2\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 360/360 [00:59<00:00,  6.07it/s]\n","100%|██████████| 32/32 [00:00<00:00, 37.27it/s]\n","  0%|          | 0/360 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Accuracy: 0.8705\n","Macro F1 Score: 0.888888888888889\n","Epoch 3\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 360/360 [00:59<00:00,  6.03it/s]\n","100%|██████████| 32/32 [00:00<00:00, 36.16it/s]\n","  4%|▍         | 16/391 [00:00<00:02, 152.54it/s]"],"name":"stderr"},{"output_type":"stream","text":["Accuracy: 0.8835\n","Macro F1 Score: 0.9411764705882353\n","Test set performance\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:08<00:00, 44.37it/s]"],"name":"stderr"},{"output_type":"stream","text":["Accuracy: 0.8588\n","Macro F1 Score: 0.85\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"EIT_ucW7ouyb"},"source":[""],"execution_count":null,"outputs":[]}]}