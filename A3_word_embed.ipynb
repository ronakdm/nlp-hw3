{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 400000\n",
      "Embeddings shape: (400000, 50)\n"
     ]
    }
   ],
   "source": [
    "embeddings = pickle.load(open(\"embeddings.p\", \"rb\")).numpy()\n",
    "vocab = pickle.load(open(\"vocab.p\", \"rb\"))\n",
    "vocab_list = pickle.load(open(\"vocab_list.p\", \"rb\"))\n",
    "\n",
    "print(\"Vocabulary size:\", len(vocab_list))\n",
    "print(\"Embeddings shape:\", embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1.** Find the most similar words and their cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(keyword):\n",
    "    idx = vocab[keyword]\n",
    "    keyword_embedding = embeddings[idx].reshape(1, -1)\n",
    "    \n",
    "    best_distance = 2\n",
    "    best_idx = -1\n",
    "    \n",
    "    sims = cosine_similarity(embeddings, keyword_embedding)\n",
    "    \n",
    "    # Get two largest elements.\n",
    "    best_idx = np.argpartition(sims.reshape(-1), -2)[-2:]\n",
    "    second_best_idx = best_idx[np.argsort(sims[best_idx], axis=0)][0, 0]\n",
    "            \n",
    "    return vocab_list[second_best_idx], sims[second_best_idx, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword: dog\n",
      "Most similar word is 'cat', with cosine distance 0.921801.\n",
      "Keyword: whale\n",
      "Most similar word is 'whales', with cosine distance 0.898683.\n",
      "Keyword: before\n",
      "Most similar word is 'after', with cosine distance 0.951184.\n",
      "Keyword: however\n",
      "Most similar word is 'although', with cosine distance 0.980139.\n",
      "Keyword: fabricate\n",
      "Most similar word is 'fabricating', with cosine distance 0.759454.\n"
     ]
    }
   ],
   "source": [
    "words = [\"dog\", \"whale\", \"before\", \"however\", \"fabricate\"]\n",
    "\n",
    "for word in words:\n",
    "    print(\"Keyword:\", word)\n",
    "    print(\"Most similar word is '%s', with cosine distance %f.\" % (most_similar(word)), end = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2.** Completing the analogy. Given analogy with words \"$w_{i_1} : w_{i_2} :: w_{j_1} : \\ ?$\", the goal is to guess word $w_{j_1}$. We guess its word embedding as $\\tilde{v}_{j_2} = -v_{i_1} + v_{j_1} + v_{i_2}$, where $v_{k}$ is the embedding of $w_k$, and find the most similar vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_analogy(word1, is_to1, word2, is_to2 = None):\n",
    "    \n",
    "    i1 = vocab[word1]\n",
    "    j1 = vocab[is_to1]\n",
    "    i2 = vocab[word2]\n",
    "    \n",
    "    v_i1 = embeddings[i1]\n",
    "    v_i2 = embeddings[i2]\n",
    "    v_j1 = embeddings[j1]\n",
    "    \n",
    "    # Guess for completing word.\n",
    "    v_j2 = -v_i1 + v_j1 + v_i2\n",
    "    \n",
    "    sims = cosine_similarity(embeddings, v_j2.reshape(1, -1))\n",
    "    \n",
    "    # Get top 6, and remove any that are included in the original words.\n",
    "    top_six = np.argpartition(sims.reshape(-1), -6)[-6:]\n",
    "    \n",
    "    best_idx = []\n",
    "    for i in top_six:\n",
    "        if i != i1 and i != j1 and i != i2:\n",
    "            best_idx.append(i)\n",
    "    best_idx = np.array(best_idx)\n",
    "    \n",
    "    # Get the top 3.\n",
    "    in_order = np.flip(best_idx[np.argsort(sims[best_idx], axis=0)][:, 0])[0:3]\n",
    "    print(\"'%s' is to '%s' as '%s' is to ____.\" % (word1, is_to1, word2))\n",
    "    for i, ind in enumerate(in_order):\n",
    "        print(\"%d. Completion: %s, similarity: %.3f.\" %(i+1, vocab_list[ind], sims[ind, 0]))\n",
    "        \n",
    "    if is_to2:\n",
    "        sim = cosine_similarity(embeddings[vocab[is_to2]].reshape(1, -1), v_j2.reshape(1, -1))\n",
    "        print(\"Similarity of '%s' and estimated word embedding: %.3f\" % (is_to2, sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'speak' is to 'speaker' as 'sing' is to ____.\n",
      "1. Completion: sang, similarity: 0.623.\n",
      "2. Completion: nateq, similarity: 0.622.\n",
      "3. Completion: lyricist, similarity: 0.602.\n",
      "Similarity of 'singer' and estimated word embedding: 0.508\n"
     ]
    }
   ],
   "source": [
    "complete_analogy(\"speak\", \"speaker\", \"sing\", \"singer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'dog' is to 'puppy' as 'cat' is to ____.\n",
      "1. Completion: puppies, similarity: 0.763.\n",
      "2. Completion: scaredy, similarity: 0.744.\n",
      "3. Completion: kitten, similarity: 0.741.\n",
      "Similarity of 'kitten' and estimated word embedding: 0.741\n"
     ]
    }
   ],
   "source": [
    "complete_analogy(\"dog\", \"puppy\", \"cat\", \"kitten\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'france' is to 'french' as 'england' is to ____.\n",
      "1. Completion: scottish, similarity: 0.868.\n",
      "2. Completion: english, similarity: 0.837.\n",
      "3. Completion: welsh, similarity: 0.806.\n",
      "Similarity of 'english' and estimated word embedding: 0.837\n"
     ]
    }
   ],
   "source": [
    "complete_analogy(\"france\", \"french\", \"england\", \"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'france' is to 'wine' as 'england' is to ____.\n",
      "1. Completion: orchard, similarity: 0.662.\n",
      "2. Completion: tasting, similarity: 0.633.\n",
      "3. Completion: tea, similarity: 0.616.\n",
      "Similarity of 'whiskey' and estimated word embedding: 0.513\n"
     ]
    }
   ],
   "source": [
    "complete_analogy(\"france\", \"wine\", \"england\", \"whiskey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
